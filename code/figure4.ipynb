{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f74e1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "token ='insert_your_token_here'\n",
    "\n",
    "from neuprint import Client\n",
    "\n",
    "c = Client('neuprint.janelia.org', dataset='manc:v1.2.1', token=token)\n",
    "c.fetch_version()\n",
    "from neuprint import fetch_adjacencies\n",
    "from neuprint import merge_neuron_properties\n",
    "from neuprint import fetch_neurons, NeuronCriteria as NC, fetch_synapses, SynapseCriteria as SC, skeleton_segments, fetch_synapse_connections\n",
    "\n",
    "from neuprint.utils import connection_table_to_matrix\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from sklearn.cluster import AgglomerativeClustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6a71c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "md_4_neurons = [15263,\n",
    " 18951,\n",
    " 21105,\n",
    " 21209,\n",
    " 21219,\n",
    " 21773,\n",
    " 21976,\n",
    " 22045,\n",
    " 22090,\n",
    " 22123,\n",
    " 22217,\n",
    " 22228,\n",
    " 22247,\n",
    " 22411,\n",
    " 22543,\n",
    " 22613,\n",
    " 22719,\n",
    " 22995,\n",
    " 23476,\n",
    " 23532,\n",
    " 23783,\n",
    " 24165,\n",
    " 24180,\n",
    " 25494,\n",
    " 25636,\n",
    " 26664,\n",
    " 101527,\n",
    " 152738,\n",
    " 154741,\n",
    " 163802]\n",
    "\n",
    "\n",
    "criteria = NC(bodyId=md_4_neurons)\n",
    "sn_df, sn_conn_df = fetch_adjacencies(criteria)\n",
    "\n",
    "neuron_info, _ = fetch_neurons(criteria)\n",
    "left_sn = neuron_info[neuron_info.instance.str.contains('_L')].bodyId.unique()\n",
    "#right_sn = sn_df[sn_df.instance_pre.str.contains('R')].bodyId_pre.unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bacaf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "order = sn_df[sn_df.bodyId.isin(md_4_neurons)][sn_df.instance.fillna('').str.contains('_L')].sort_values('instance', ascending=True).bodyId.unique()\n",
    "\n",
    "dv = [163802,  23476,  23783,  24180,  26664,  21976,  22123,\n",
    "       23532,  18951,  25636,  21773,  22045,  22090, 154741,  22247]\n",
    "relation = sn_conn_df[sn_conn_df.bodyId_post.isin(left_sn)].pivot_table(index='bodyId_pre', columns='bodyId_post', \n",
    "                                               values='weight', aggfunc='sum').fillna(0)\n",
    "relation = relation[dv].reindex(dv)\n",
    "\n",
    "r = relation.drop_duplicates().copy()\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(6,6))\n",
    "#plt.rcParams[\"font.size\"] = 5\n",
    "sns.heatmap(r, vmin=0,vmax=20, square=True, cmap='Greys')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325d614e",
   "metadata": {},
   "outputs": [],
   "source": [
    "order = sn_df[sn_df.bodyId.isin(md_4_neurons)][sn_df.instance.fillna('').str.contains('_L')].sort_values('instance', ascending=True).bodyId.unique()\n",
    "\n",
    "ap = [18951, 23532, 25636, 163802, \n",
    "      21773, 22045, 22090, 23476, 23783, 24180, 26664, 154741,\n",
    "      21976, 22123, 22247]\n",
    "relation = sn_conn_df[sn_conn_df.bodyId_post.isin(left_sn)].pivot_table(index='bodyId_pre', columns='bodyId_post', \n",
    "                                               values='weight', aggfunc='sum').fillna(0)\n",
    "relation = relation[ap].reindex(ap)\n",
    "\n",
    "r = relation.drop_duplicates().copy()\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(6,6))\n",
    "#plt.rcParams[\"font.size\"] = 5\n",
    "sns.heatmap(r, vmin=0,vmax=20, square=True, cmap='Greys')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9097e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "synapses = fetch_synapse_connections(NC(bodyId=left_sn))\n",
    "\n",
    "synapses['pre_point'] = synapses.apply(lambda row: [row['x_pre'], row['y_pre'], row['z_pre']], axis=1)\n",
    "synapses['post_point'] = synapses.apply(lambda row: [row['x_post'], row['y_post'], row['z_post']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa47812",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filt_syn_df(syn_df, syn_thresh):\n",
    "    \n",
    "    indices_to_include = []\n",
    "\n",
    "    # Find unique presynaptic neurons\n",
    "    unique_pre_ids = syn_df.bodyId_pre.unique().tolist()\n",
    "\n",
    "    # Loop through presynaptic neurons\n",
    "    for i in unique_pre_ids:       \n",
    "        pre_df = syn_df[syn_df.bodyId_pre == i]\n",
    "        # Find unique postsynaptic neurons targeted by i-th presynaptic neuron\n",
    "        unique_post_ids = pre_df.bodyId_post.unique().tolist() \n",
    "        # Loop through postsynaptic neurons\n",
    "        for j in unique_post_ids:    \n",
    "            # Is number of synapses onto j-th postsynaptic neuron larger than or equal to syn_thresh? \n",
    "            if sum(pre_df.bodyId_post == j) >= syn_thresh: \n",
    "                # Get indices (rows)\n",
    "                indices = pre_df.index.values[pre_df.bodyId_post == j]       \n",
    "                # Loop through indices \n",
    "                for k in indices:\n",
    "                    # Append each index separately to avoid lists within list\n",
    "                    indices_to_include.append(k)\n",
    "\n",
    "    # Sort indices in ascending order\n",
    "    indices_to_include.sort()\n",
    "    \n",
    "    # Create new syn_df with only connections above syn_thresh\n",
    "    syn_df_filt = syn_df.loc[indices_to_include]\n",
    "    \n",
    "    return syn_df_filt\n",
    "\n",
    "#functions adapted from FANC\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "def get_networkx_graph(syn_df, source = 'bodyId_pre', \n",
    "                       target = 'bodyId_post', edge_attr = 'count'):\n",
    "    \n",
    "    \n",
    "    edge_df = syn_df.groupby([source, target]).size().sort_values(ascending=False).reset_index(name=edge_attr)\n",
    "        \n",
    "    graph = nx.from_pandas_edgelist(edge_df, source=source, \n",
    "                                  target=target, edge_attr=edge_attr)\n",
    "    return graph\n",
    "\n",
    "def get_edge_df(syn_df, source = 'bodyId_pre', \n",
    "                target = 'bodyId_post', edge_attr = 'count'):\n",
    "\n",
    "    \n",
    "    edge_df = syn_df.groupby([source, target]).size().sort_values(ascending=False).reset_index(name=edge_attr)\n",
    "    \n",
    "    \n",
    "    return edge_df\n",
    "\n",
    "def get_asymm_conn_mat(syn_df,  weight='count'):\n",
    "    \n",
    "    g = get_networkx_graph(syn_df,  edge_attr=weight)\n",
    "    \n",
    "    edge_df = get_edge_df(syn_df,  edge_attr=weight)\n",
    "    \n",
    "    targets = list(edge_df.bodyId_post.unique())\n",
    "    sources = list(edge_df.bodyId_pre.unique())\n",
    "    \n",
    "    # Get indices for g.nodes indicating which node IDs are sources and which are targets\n",
    "    source_ix, target_ix = [], []\n",
    "    source_ids, target_ids = [], []\n",
    "    for ix, i in enumerate(g.nodes()):\n",
    "        if i in sources:\n",
    "            source_ix.append(ix) \n",
    "            source_ids.append(i) \n",
    "        if i in targets:\n",
    "            target_ix.append(ix)\n",
    "            target_ids.append(i)\n",
    "\n",
    "    conn_mat = nx.to_numpy_array(g, weight=weight) # Full connectivity matrix\n",
    "#     conn_mat = nx.DiGraph(g, weight=weight)\n",
    "    asymm_conn_mat = conn_mat[source_ix,:]\n",
    "    asymm_conn_mat = asymm_conn_mat[:,target_ix]\n",
    "    \n",
    "    return asymm_conn_mat, source_ids, target_ids\n",
    "\n",
    "def get_clustered_order(sim_mat, distance_threshold = 0, \n",
    "                       n_clusters = None, **kwargs):\n",
    "    \n",
    "    model = AgglomerativeClustering(distance_threshold=distance_threshold, n_clusters=None).fit(sim_mat)\n",
    "    #create the counts of samples under each node\n",
    "    counts = np.zeros(model.children_.shape[0])\n",
    "    n_samples = len(model.labels_)\n",
    "    for i, merge in enumerate(model.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1  #leaf node\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "\n",
    "    linkage_matrix = np.column_stack(\n",
    "        [model.children_, model.distances_, counts]\n",
    "    ).astype(float)\n",
    "\n",
    "    #plot the corresponding dendrogram\n",
    "    dendrogram(linkage_matrix, **kwargs)\n",
    "    dend_dict = dendrogram(linkage_matrix, **kwargs)\n",
    "    \n",
    "    #sorted order of indices found through clustering\n",
    "    clustered_order = dend_dict['ivl']\n",
    "    \n",
    "    parsed_order = []\n",
    "    for c in clustered_order:\n",
    "        if '(' in c:\n",
    "            c = c.split('(')[1]\n",
    "            c = c.split(')')[0]\n",
    "        parsed_order.append(int(c))\n",
    "    \n",
    "    return parsed_order\n",
    "\n",
    "def get_sim_matrix(syn_df, asymmetric = True, weight='count'):\n",
    "    \n",
    "    #get weighted connectivity matrix from networkx graph\n",
    "    if asymmetric:\n",
    "        conn_mat, sources, targets = get_asymm_conn_mat(syn_df, weight=weight)\n",
    "    else:\n",
    "        g = get_networkx_graph(syn_df, edge_attr=weight)\n",
    "        conn_mat = nx.to_numpy_array(g, weight=weight)\n",
    "     \n",
    "    conn_mat = np.asarray(conn_mat) #np.matrix usage is deprecated; convert to np.array\n",
    "\n",
    "    #calculate cosine similarity\n",
    "    sim_mat = cosine_similarity(conn_mat)\n",
    "    \n",
    "    return conn_mat, sim_mat\n",
    "\n",
    "def reorder_by_cosine(syn_df, weight ='count', \n",
    "                      distance_threshold = 0, \n",
    "                      n_clusters = None,\n",
    "                      asymmetric = True,\n",
    "                      column_order = 'cosine'):\n",
    "\n",
    "    #get weighted connectivity matrix from networkx graph\n",
    "    if asymmetric:\n",
    "        conn_mat, sources, targets = get_asymm_conn_mat(syn_df, weight=weight) \n",
    "    else:\n",
    "        g = get_networkx_graph(syn_df, edge_attr=weight)\n",
    "        conn_mat = nx.to_numpy_array(g, weight=weight)\n",
    "        \n",
    "    conn_mat = np.asarray(conn_mat) #np.matrix usage is deprecated; convert to np.array\n",
    "    \n",
    "    #calculate cosine similarity\n",
    "    sim_mat = cosine_similarity(conn_mat)\n",
    "    \n",
    "    #clustering based on cosine similarity\n",
    "    row_ordered = get_clustered_order(sim_mat, distance_threshold=distance_threshold, n_clusters=n_clusters) \n",
    "    \n",
    "    if asymmetric==True and column_order == 'cosine':\n",
    "        column_sim_mat = cosine_similarity(conn_mat.T)\n",
    "        column_ordered = get_clustered_order(column_sim_mat, distance_threshold=distance_threshold, n_clusters=n_clusters)\n",
    "    elif asymmetric == False and column_order == 'cosine':\n",
    "        column_ordered = row_ordered\n",
    "    else:\n",
    "        column_ordered = column_order\n",
    "    \n",
    "    #reordering connectivity matrix \n",
    "    conn_mat = conn_mat[row_ordered, :]\n",
    "    conn_mat = conn_mat[:, column_ordered]\n",
    "\n",
    "    #reordering source similarity matrix\n",
    "    sim_mat = sim_mat[row_ordered, :]\n",
    "    sim_mat = sim_mat[:, row_ordered] \n",
    "        \n",
    "    #reordering source and target IDs\n",
    "    sources_ordered = [sources[i] for i in row_ordered] \n",
    "    targets_ordered = [targets[i] for i in column_ordered] \n",
    "     \n",
    "    return conn_mat, sim_mat, sources_ordered, targets_ordered\n",
    "\n",
    "\n",
    "def reorder_by_custom_order(syn_df, \n",
    "                            custom_row_order=None, \n",
    "                            custom_col_order=None, \n",
    "                            weight='count', \n",
    "                            asymmetric=True):\n",
    "    \"\"\"\n",
    "    Reorders the connectivity and similarity matrices based on custom-defined row and column orders.\n",
    "    \n",
    "    Parameters:\n",
    "    - syn_df: DataFrame with synapse information.\n",
    "    - custom_row_order: List of source neuron IDs in desired row order.\n",
    "    - custom_col_order: List of target neuron IDs in desired column order.\n",
    "    - weight: Edge attribute used for weights (default 'count').\n",
    "    - asymmetric: Whether to treat the matrix as asymmetric (default True).\n",
    "    \n",
    "    Returns:\n",
    "    - conn_mat_reordered: Reordered connectivity matrix.\n",
    "    - sim_mat_reordered: Reordered cosine similarity matrix.\n",
    "    - sources_ordered: Ordered list of source neuron IDs.\n",
    "    - targets_ordered: Ordered list of target neuron IDs.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get connectivity matrix and source/target IDs\n",
    "    if asymmetric:\n",
    "        conn_mat, sources, targets = get_asymm_conn_mat(syn_df, weight=weight)\n",
    "    else:\n",
    "        g = get_networkx_graph(syn_df, edge_attr=weight)\n",
    "        conn_mat = nx.to_numpy_array(g, weight=weight)\n",
    "        sources = targets = list(g.nodes())\n",
    "\n",
    "    conn_mat = np.asarray(conn_mat)\n",
    "    sim_mat = cosine_similarity(conn_mat)\n",
    "\n",
    "    # Build index maps for custom ordering\n",
    "    if custom_row_order is not None:\n",
    "        row_indices = [sources.index(i) for i in custom_row_order]\n",
    "    else:\n",
    "        row_indices = list(range(len(sources)))\n",
    "\n",
    "    if custom_col_order is not None:\n",
    "        col_indices = [targets.index(j) for j in custom_col_order]\n",
    "    else:\n",
    "        col_indices = list(range(len(targets)))\n",
    "\n",
    "    # Reorder matrices\n",
    "    conn_mat_reordered = conn_mat[row_indices, :][:, col_indices]\n",
    "    sim_mat_reordered = sim_mat[row_indices, :][:, row_indices]\n",
    "\n",
    "    # Get the ordered sources/targets\n",
    "    sources_ordered = [sources[i] for i in row_indices]\n",
    "    targets_ordered = [targets[j] for j in col_indices]\n",
    "\n",
    "    return conn_mat_reordered, sim_mat_reordered, sources_ordered, targets_ordered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f839905",
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_thresh = 4\n",
    "synapse_df = filt_syn_df(synapses, syn_thresh)\n",
    "\n",
    "filtered_synapse_df = synapse_df[synapse_df['bodyId_post'].isin(sn_conn_df[sn_conn_df.weight>=syn_thresh].bodyId_post.unique())]\n",
    "#plt.figure(figsize=(20,20))\n",
    "\n",
    "conn_mat, sim_mat, pre_ids_ordered, post_ids_ordered = reorder_by_cosine(filtered_synapse_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79444a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "conn_mat, sim_mat, src_ordered, tgt_ordered = reorder_by_custom_order(\n",
    "    filtered_synapse_df,\n",
    "    custom_row_order=dv,\n",
    "    custom_col_order=dv\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95ba9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('talk')\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "sns.heatmap(sim_mat, vmin=0.5, square=True, cmap='Greys', cbar=False)\n",
    "plt.xlabel('md')\n",
    "plt.ylabel('md')\n",
    "plt.xticks(rotation = 90)\n",
    "plt.rcParams.update({'font.size': 11})\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "manc_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
