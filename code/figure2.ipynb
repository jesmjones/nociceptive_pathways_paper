{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df325dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats, signal\n",
    "from scipy.fft import fft, fftfreq\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm, trange\n",
    "#from google.colab import drive\n",
    "import glob\n",
    "import pyarrow.parquet as pq\n",
    "#drive.mount('/content/drive')\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams.update(plt.rcParamsDefault)\n",
    "\n",
    "plt.rcParams['pdf.fonttype']= 42\n",
    "plt.rcParams['svg.fonttype'] = 'none'\n",
    "plt.rcParams[\"font.family\"] = \"Arial\"\n",
    "plt.rcParams[\"font.size\"] = 11\n",
    "plt.rcParams[\"text.color\"] = 'black'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fdbd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 'H:/.shortcut-targets-by-id/10pxdlRXtzFB-abwDGi0jOGOFFNm3pmFK/Tuthill Lab Shared/Jessica/'\n",
    "data1 = pd.read_csv(p + 'all_data_headvsheadless_pub.pq')\n",
    "data2 = pd.read_csv(p + 'all_head_on_fictracdata.pq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399a609a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heading_histogram(angles_deg, bins=36, bottom=0, alpha=0.5, color='orchid', edgecolor='k', proportion=False, hide_y=False, set_bottom=False):\n",
    "    angles_rad = np.deg2rad(angles_deg % 360)\n",
    "    counts, bin_edges = np.histogram(angles_rad, bins=bins, range=(0, 2*np.pi))\n",
    "    bin_width = bin_edges[1] - bin_edges[0]\n",
    "    \n",
    "    if proportion:\n",
    "        counts = counts / counts.sum()\n",
    "\n",
    "    if set_bottom:\n",
    "        bottom = np.max(counts)\n",
    "\n",
    "    fig = plt.figure(figsize=[3, 3])\n",
    "    ax = fig.add_subplot(1, 1, 1, projection=\"polar\")\n",
    "    ax.bar(bin_edges[:-1], counts, width=bin_width, bottom=bottom, alpha=alpha, color=color, edgecolor=edgecolor, linewidth=.3)\n",
    "    \n",
    "    if hide_y:\n",
    "        ax.set_yticklabels([])\n",
    "        ax.yaxis.grid(False)\n",
    "\n",
    "\n",
    "    return fig, ax\n",
    "\n",
    "f, ax = plot_heading_histogram(data2[(data2.dir!='control')&\n",
    "                                           (data2.stimlen!=0.0)&\n",
    "                                           (data2.fictrac_delta_rot_lab_y_mms.between(1,30))&\n",
    "                                           (data2.fnum.between(0.499, 1.5))]\n",
    "                                            ['fictrac_heading_deg'].values, \n",
    "                                           color=\"#000000\",\n",
    "                                           bins=60, bottom=0, proportion=True, hide_y=True, set_bottom=True)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18b0a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "def compare_heading_vectors(group1_angles_deg, group2_angles_deg, bins=36):\n",
    "    \"\"\"\n",
    "    Compare the mean resultant vector directions of two groups of angles (in degrees).\n",
    "    Returns the mean direction and resultant length for each group.\n",
    "    \"\"\"\n",
    "    def mean_vector(angles_deg):\n",
    "        angles_rad = np.deg2rad(angles_deg % 360)\n",
    "        # Compute mean resultant vector\n",
    "        x = np.cos(angles_rad).mean()\n",
    "        y = np.sin(angles_rad).mean()\n",
    "        mean_angle = np.arctan2(y, x)\n",
    "        mean_angle_deg = np.rad2deg(mean_angle) % 360\n",
    "        r = np.sqrt(x**2 + y**2)\n",
    "        return mean_angle_deg, r\n",
    "\n",
    "    mean1, r1 = mean_vector(group1_angles_deg)\n",
    "    mean2, r2 = mean_vector(group2_angles_deg)\n",
    "\n",
    "    print(f\"Group 1: Mean direction = {mean1:.2f}°, Resultant length = {r1:.2f}\")\n",
    "    print(f\"Group 2: Mean direction = {mean2:.2f}°, Resultant length = {r2:.2f}\")\n",
    "\n",
    "    return (mean1, r1), (mean2, r2)\n",
    "\n",
    "# Example usage:\n",
    "(mean1, r1), (mean2, r2) = compare_heading_vectors(\n",
    "    data2[(data2.dir=='control')&\n",
    "                (data2.stimlen!=0.0)&\n",
    "                (data2.fictrac_delta_rot_lab_y_mms.between(1,30))&\n",
    "                (data2.fnum.between(0.499, 1.5))]\n",
    "                ['fictrac_heading_deg'].values,\n",
    "    data2[(data2.dir!='control')&\n",
    "                (data2.stimlen!=0.0)&\n",
    "                (data2.fictrac_delta_rot_lab_y_mms.between(1,30))&\n",
    "                (data2.fnum.between(0.499, 1.5))]\n",
    "                ['fictrac_heading_deg'].values\n",
    ")\n",
    "\n",
    "# Compute the difference in means\n",
    "mean_diff = abs(mean1 - mean2)\n",
    "print(f\"Difference in resultant means: {mean_diff:.4f}°\")\n",
    "\n",
    "# Get the angle distributions (in degrees)\n",
    "group1 = data2[(data2.dir=='control')&\n",
    "                     (data2.stimlen!=0.0)&\n",
    "                     (data2.fictrac_delta_rot_lab_y_mms.between(1,30))&\n",
    "                     (data2.fnum.between(0.499, 1.5))]['fictrac_heading_deg'].values\n",
    "\n",
    "group2 = data2[(data2.dir!='control')&\n",
    "                     (data2.stimlen!=0.0)&\n",
    "                     (data2.fictrac_delta_rot_lab_y_mms.between(1,30))&\n",
    "                     (data2.fnum.between(0.499, 1.5))]['fictrac_heading_deg'].values\n",
    "\n",
    "# A better test for comparing circular (angle) data is the Watson-Williams test or the nonparametric Watson's U^2 test.\n",
    "# You can use scipy.stats.circmean/circstd for summary, but for statistical comparison, use pycircstat if available.\n",
    "# Example with pycircstat (if installed):\n",
    "\n",
    "# import pycircstat\n",
    "# import pycircstat.tests\n",
    "# p_val = pycircstat.tests.watson_williams(np.deg2rad(group1), np.deg2rad(group2))\n",
    "# print(f\"Watson-Williams test p = {p_val:.4f}\")\n",
    "\n",
    "# If pycircstat is not available, you can use scipy.stats.circmean for summary statistics:\n",
    "circmean1 = stats.circmean(np.deg2rad(group1), high=2*np.pi)\n",
    "circmean2 = stats.circmean(np.deg2rad(group2), high=2*np.pi)\n",
    "print(f\"Group 1 circular mean: {np.rad2deg(circmean1):.2f}°\")\n",
    "print(f\"Group 2 circular mean: {np.rad2deg(circmean2):.2f}°\")\n",
    "# compare two means with t-test (not ideal for circular data)\n",
    "t_stat, p_val = ttest_ind(group1, group2)\n",
    "print(f\"T-test p = {p_val:.4f}\")\n",
    "print(\"significant?:\",p_val<=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc42190",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[1.5,2.0])\n",
    "sns.lineplot(data=data2[(data2.stimlen.isin([1.0]))&(data2.dir.isin(['middle', 'control']))&(data.fictrac_delta_rot_lab_y_mms<=30)], \n",
    "                   hue='dir', x='fnum', y='fictrac_delta_rot_lab_y_mms', errorbar=('ci', 95),\n",
    "                  legend=False,\n",
    "                  palette=[ \"#3CC443\",'#000000'])\n",
    "\n",
    "\n",
    "plt.yticks([0,5,10])\n",
    "plt.xticks([0,0.5,1,1.5,2])\n",
    "plt.ylabel('Velocity (mm/s)')\n",
    "plt.xlabel('Time (s)')\n",
    "sns.despine(trim=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a0ab3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62270e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to classify behavior based on velocity in head-on\n",
    "def classify_behavior(velocity):\n",
    "    if velocity < 1:\n",
    "        return 'standing'\n",
    "\n",
    "    elif velocity >= 1 and velocity < 2:\n",
    "        return 'grooming/other'\n",
    "    \n",
    "    elif velocity >= 2 and velocity <= 35:\n",
    "        return 'walking'\n",
    "\n",
    "    elif velocity > 35:\n",
    "        return 'jumping'\n",
    "    else:\n",
    "        return 'unknown'  # Handle any unexpected values\n",
    "# Apply the function to create the 'behavior' column\n",
    "splits = ['5_0', '7_0', '8_0', '9_0']\n",
    "mid = data1[data1.date_parsed=='4.28.22']\n",
    "#mid = mid[~mid.fly.isin(splits)]\n",
    "d = mid.copy()\n",
    "d['behavior'] = d['fictrac_delta_rot_lab_y_mms'].apply(classify_behavior)\n",
    "\n",
    "behavior_counts_head = d.groupby(['fnum','stimlen','behavior', 'fly']).size().reset_index(name='count')\n",
    "total_counts_head = d.groupby(['fnum','stimlen', 'fly']).size().reset_index(name='total_count')\n",
    "# Merge the behavior counts with total counts\n",
    "merged_head = pd.merge(behavior_counts_head, total_counts_head, on=['fnum','stimlen', 'fly'])\n",
    "# Calculate the probability of each behavior at each frame\n",
    "merged_head['probability'] = merged_head['count'] / merged_head['total_count']\n",
    "merged_head['probability'] = merged_head['probability'] *100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8358d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(2.1, 1.5))\n",
    "sns.lineplot(data=d[d.behavior.isin(['jumping'])][d.stimlen!=0.0], \n",
    "           x='fnum', \n",
    "           y='jumping_prob', \n",
    "           hue='behavior', \n",
    "           palette=[\"#000000\"],\n",
    "           #errorbar=None,\n",
    "           legend=False,)\n",
    "plt.xticks(np.arange(0, 2.1, 0.5))\n",
    "plt.yticks(np.arange(0.0, 0.26, .05))\n",
    "\n",
    "sns.despine(trim=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f96e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stimulus onset (if known/fixed)\n",
    "stimulus_onset_frame = 0.499  # Example value\n",
    "\n",
    "# Filter to only relevant data after stimulus onset\n",
    "df_after_stimulus = d[d[\"fnum\"] >= stimulus_onset_frame]\n",
    "\n",
    "# Group by fullfile and fly\n",
    "latency_results = []\n",
    "\n",
    "for (fullfile, fly), group in df_after_stimulus.groupby([\"fullfile\", \"fly\"]):\n",
    "    jumping_rows = group[group[\"behavior\"] == \"jumping\"]\n",
    "    \n",
    "    if not jumping_rows.empty:\n",
    "        first_jump_frame = jumping_rows[\"fnum\"].min()\n",
    "        latency = first_jump_frame - stimulus_onset_frame\n",
    "    else:\n",
    "        latency = None  # Or some sentinel value like -1 or np.nan\n",
    "\n",
    "    latency_results.append({\n",
    "        \"fullfile\": fullfile,\n",
    "        \"fly\": fly,\n",
    "        \"latency_to_jump\": latency\n",
    "    })\n",
    "\n",
    "# Convert results to DataFrame\n",
    "latency_df = pd.DataFrame(latency_results)\n",
    "\n",
    "latency_df.dropna()\n",
    "fig=plt.figure(figsize=[1,2])\n",
    "sns.stripplot(data=latency_df.dropna(),  \n",
    "              y='latency_to_jump',\n",
    "              size=3,\n",
    "              alpha=.5,\n",
    "              color='black')\n",
    "sns.despine(trim=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c0d4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(2.1, 1.5))\n",
    "sns.lineplot(data=d[d.behavior.isin(['walking'])][d.stimlen!=0.0], \n",
    "           x='fnum', \n",
    "           y='walking_prob', \n",
    "           hue='behavior', \n",
    "           palette=[\"#2868c7\"],\n",
    "           #errorbar=None,\n",
    "           legend=False,)\n",
    "plt.xticks(np.arange(0, 2.1, 0.5))\n",
    "plt.yticks([0.5,0.55,.6,.65,])\n",
    "plt.ylim(0.5, .65)\n",
    "sns.despine(trim=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2039da8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stimulus onset (if known/fixed)\n",
    "stimulus_onset_frame = 0.499  # Example value\n",
    "\n",
    "# Filter to only relevant data after stimulus onset\n",
    "df_after_stimulus = d[d[\"fnum\"] >= stimulus_onset_frame]\n",
    "\n",
    "# Group by fullfile and fly\n",
    "latency_results = []\n",
    "\n",
    "for (fullfile, fly), group in df_after_stimulus.groupby([\"fullfile\", \"fly\"]):\n",
    "    walking_rows = group[group[\"behavior\"] == \"walking\"]\n",
    "    \n",
    "    if not walking_rows.empty:\n",
    "        first_walking_frame = walking_rows[\"fnum\"].min()\n",
    "        latency = first_walking_frame - stimulus_onset_frame\n",
    "    else:\n",
    "        latency = None  # Or some sentinel value like -1 or np.nan\n",
    "\n",
    "    latency_results.append({\n",
    "        \"fullfile\": fullfile,\n",
    "        \"fly\": fly,\n",
    "        \"latency_to_walk\": latency\n",
    "    })\n",
    "\n",
    "# Convert results to DataFrame\n",
    "latency_df = pd.DataFrame(latency_results)\n",
    "\n",
    "latency_df.dropna()\n",
    "fig=plt.figure(figsize=[1,2])\n",
    "sns.stripplot(data=latency_df.dropna(),  \n",
    "              y='latency_to_walk',\n",
    "              size=3,\n",
    "              alpha=.5,\n",
    "              color=\"#2868c7\",\n",
    "                )\n",
    "sns.despine(trim=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0b2e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(2.1, 1.5))\n",
    "sns.lineplot(data=d[d.behavior.isin(['grooming/other'])][d.stimlen!=0.0], \n",
    "           x='fnum', \n",
    "           y='t3_grooming_prob', \n",
    "           hue='behavior', \n",
    "           palette=[\"#fc8be6\"],\n",
    "           #errorbar=None,\n",
    "           legend=False,)\n",
    "plt.xticks(np.arange(0, 2.1, 0.5))\n",
    "plt.yticks([0.0, 0.1, 0.2])\n",
    "\n",
    "sns.despine(trim=True)\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(2.1, 1.5))\n",
    "sns.lineplot(data=d[d.behavior.isin(['standing'])][d.stimlen!=0.0], \n",
    "           x='fnum', \n",
    "           y='standing_prob', \n",
    "           hue='behavior', \n",
    "           palette=[\"#B6B4B4\"],\n",
    "           #errorbar=None,\n",
    "           legend=False,)\n",
    "plt.xticks(np.arange(0, 2.1, 0.5))\n",
    "plt.yticks([0.05, .1,.15,.2])\n",
    "plt.ylim(0.05, .25)\n",
    "sns.despine(trim=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36900498",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "behavior_map = {behavior: idx for idx, behavior in enumerate(d['behavior'].unique())}\n",
    "d['behavior_num'] = d['behavior'].map(behavior_map)\n",
    "\n",
    "\n",
    "# Group by 'genotype' and 'time', then count occurrences of each behavior\n",
    "grouped = d.groupby(['filename','fly', 'fnum', 'behavior']).size().reset_index(name='count')\n",
    "\n",
    "# Calculate total counts for each genotype and time combination\n",
    "total_counts = grouped.groupby(['filename','fly', 'fnum'])['count'].sum().reset_index(name='total_count')\n",
    "\n",
    "# Merge total counts back to grouped dataframe to calculate probabilities\n",
    "grouped = pd.merge(grouped, total_counts, on=['filename','fly', 'fnum'])\n",
    "\n",
    "# Calculate probability as count divided by total_count\n",
    "grouped['probability'] = grouped['count'] / grouped['total_count']\n",
    "\n",
    "# Select relevant columns for the final dataframe\n",
    "probabilities_df = grouped[['filename','fly', 'fnum', 'behavior', 'probability']]\n",
    "\n",
    "\n",
    "# Sample DataFrame (Replace this with your actual DataFrame)\n",
    "# df = pd.read_csv('your_data.csv')\n",
    "\n",
    "# Step 1: Convert 'behavior' to numerical representation\n",
    "behavior_map = {behavior: idx for idx, behavior in enumerate(probabilities_df['behavior'].unique())}\n",
    "probabilities_df['behavior_num'] = probabilities_df['behavior'].map(behavior_map)\n",
    "probabilities_df = probabilities_df[probabilities_df.behavior!='unknown']\n",
    "# Step 2: Plot Ethograms for Each Unique file_path and fly_id\n",
    "unique_file_paths = probabilities_df['filename'].unique()\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "#{'walking': 0, 'grooming/other': 1, 'standing': 2, 'jumping': 3}\n",
    "\n",
    "custom_cmap = ListedColormap([\"#2868c7\", \"#e268f2\", \"#ffffff\", \"#000000\", \"#ffffff\"])\n",
    "\n",
    "\n",
    "fly_df = probabilities_df[probabilities_df.filename.str.contains('-1 sec')]\n",
    "unique_file_ids = np.random.permutation(fly_df['filename'].unique())\n",
    "fly_df['filename'] = pd.Categorical(fly_df['filename'], categories=unique_file_ids, ordered=True)\n",
    "\n",
    "pivot_df = fly_df.pivot_table(index='filename', columns='fnum', values='behavior_num', aggfunc=lambda x: x.mode()[0])\n",
    "\n",
    "fig = plt.figure(figsize=(5.5, len(unique_file_ids) * 0.05))\n",
    "sns.heatmap(pivot_df, cmap=custom_cmap, vmax=4, cbar_kws={'label': 'Behavior'}, xticklabels=150, yticklabels=1)\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Fly ID')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2e51d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a19fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "headless_data = data1[data1['head'].isin(['OFF', 'other'])]\n",
    "headless_data = headless_data[~headless_data.fly.isin(['10_0','11_0', '12_0', '13_0', '14_0', '15_0', '6_0', '7_0', '8_0', '9_0'])]\n",
    "headless_data = headless_data[headless_data.date_parsed.isin(['3.24.23', '3.28.23', '5.11.22'])]\n",
    "\n",
    "\n",
    "from scipy.signal import find_peaks\n",
    "def detect_peaks(series, min_val, max_val, time_series=None):\n",
    "    \"\"\"\n",
    "    Detect peaks in a time series that exceed a threshold above a baseline \n",
    "    calculated from the first 0.25 seconds of the data.\n",
    "    \n",
    "    Parameters:\n",
    "        series (pd.Series): The data series (e.g., joint angle or kinematic data).\n",
    "        min_val (float): Minimum peak height (optional).\n",
    "        max_val (float): Maximum peak height (optional).\n",
    "        time_series (pd.Series or np.ndarray): Optional time values aligned with `series`.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Indices of peaks that exceed 25 units above baseline.\n",
    "    \"\"\"\n",
    "    # def detect_peaks(series, min_val, max_val):\n",
    "    # \"\"\"\n",
    "    # Detect if a time series oscillates between given min and max values.\n",
    "    # \"\"\"\n",
    "    # # Find peaks and troughs\n",
    "    # if min_val and max_val  is None:\n",
    "    #      peaks, _ = find_peaks(series)\n",
    "    # else:\n",
    "    #     peaks, _ = find_peaks(series, height=(min_val, max_val))\n",
    "    # #troughs, _ = find_peaks(-series, distance=30)\n",
    "    \n",
    "    # # Check if peaks and troughs fall within the desired range\n",
    "    # return peaks\n",
    "    # Find all candidate peaks\n",
    "    if min_val is None and max_val is None:\n",
    "        peaks, properties = find_peaks(series)\n",
    "    else:\n",
    "        peaks, properties = find_peaks(series, height=(min_val, max_val))\n",
    "    \n",
    "    # Determine baseline from first 0.25 seconds\n",
    "    if time_series is not None:\n",
    "        baseline_mask = time_series <= 0.25\n",
    "        baseline = series[baseline_mask].mean()\n",
    "    else:\n",
    "        # Assume uniform sampling and take first 25% of 1 second (adjust if needed)\n",
    "        baseline_window = int(len(series) * 0.25)  # Modify if sampling rate known\n",
    "        baseline = series.iloc[:baseline_window].mean()\n",
    "\n",
    "    # Keep only peaks where height > baseline + 25\n",
    "    peak_heights = series[peaks]\n",
    "    valid_peak_indices = peaks[peak_heights > (baseline + 25)]\n",
    "    \n",
    "    return valid_peak_indices\n",
    "\n",
    "def classify_behavior(peaks_detected):\n",
    "    \"\"\"\n",
    "    Classify behavior based on detected peaks in leg kinematics.\n",
    "    \"\"\"\n",
    "    kinematic_cols = ['L1_FTi', 'L2_FTi', 'L3_FTi', 'R1_FTi', 'R2_FTi', 'R3_FTi']\n",
    "    \n",
    "    # Check which columns have peaks\n",
    "    peaks_present = {col: len(peaks_detected[col]) > 0 for col in kinematic_cols}\n",
    "    \n",
    "    # Determine behavior based on detected peaks\n",
    "    # if all(peaks_present[col] for col in ['L1_FTi', 'R1_FTi', 'L3_FTi', 'R3_FTi']):\n",
    "    #     return 'walking'\n",
    "    if peaks_present['L3_FTi'] and peaks_present['R3_FTi'] and not any(peaks_present[col] for col in ['L1_FTi', 'R1_FTi', 'L2_FTi', 'R2_FTi']):\n",
    "        return 'grooming'\n",
    "    elif peaks_present['L2_FTi'] and peaks_present['R2_FTi'] and not any(peaks_present[col] for col in ['L1_FTi', 'R1_FTi', 'L3_FTi', 'R3_FTi']):\n",
    "        return 'grooming'\n",
    "    elif sum(peaks_present[col] for col in kinematic_cols) == 1:\n",
    "        return 'kicking'\n",
    "    else:\n",
    "        return 'standing'\n",
    "\n",
    "def process_chunks(trial_df, min_peak_height, max_peak_height):\n",
    "    \"\"\"\n",
    "    Process each chunk to classify behavior based on peak detection.\n",
    "    \"\"\"\n",
    "    chunk_behavior = []\n",
    "    \n",
    "    # Identify continuous chunks of time where peaks are detected\n",
    "    kinematic_cols = ['L1_FTi', 'L2_FTi', 'L3_FTi', 'R1_FTi', 'R2_FTi', 'R3_FTi']\n",
    "    \n",
    "    for start in range(0, len(trial_df), 35):  # Adjust chunk size as needed\n",
    "        end = min(start + 35, len(trial_df))  # Define chunk end\n",
    "        chunk_df = trial_df.iloc[start:end]\n",
    "        \n",
    "        # Detect peaks in each column within the chunk\n",
    "        # peaks_detected = {col: detect_peaks(chunk_df[col], min_peak_height, max_peak_height) for col in kinematic_cols}\n",
    "        # Ensure chunk_df index is reset so peak indices align with chunk_df rows\n",
    "        chunk_df = chunk_df.reset_index(drop=True)\n",
    "        peaks_detected = {\n",
    "            col: detect_peaks(chunk_df[col], min_peak_height, max_peak_height, time_series=chunk_df['fnum']) \n",
    "            for col in kinematic_cols\n",
    "        }\n",
    "        \n",
    "        # Classify behavior for this chunk\n",
    "        behavior = classify_behavior(peaks_detected)\n",
    "        \n",
    "        # Assign behavior to chunk\n",
    "        chunk_behavior.extend([behavior] * len(chunk_df))\n",
    "    \n",
    "    return chunk_behavior\n",
    "\n",
    "def apply_behavior_classification(df, min_peak_height, max_peak_height):\n",
    "    \"\"\"\n",
    "    Apply behavior classification to each trial in the DataFrame.\n",
    "    \"\"\"\n",
    "    df['behavior'] = np.nan  # Initialize behavior column\n",
    "    \n",
    "    for trial in df['filename'].unique():\n",
    "        trial_df = df[df['filename'] == trial]\n",
    "        \n",
    "        # Process chunks to classify behavior\n",
    "        chunk_behavior = process_chunks(trial_df, min_peak_height, max_peak_height)\n",
    "        \n",
    "        # Update the DataFrame with classified behavior\n",
    "        df.loc[df['filename'] == trial, 'behavior'] = chunk_behavior\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "min_val = 50\n",
    "max_val = 190\n",
    "\n",
    "df_headless = apply_behavior_classification(headless_data, min_peak_height=None, max_peak_height=None)\n",
    "df_headless['condition'] = 'e'\n",
    "df_headless.loc[df_headless.date_parsed=='5.11.22', 'condition'] = 'c'\n",
    "\n",
    "behavior_counts_headless = df_headless.groupby(['fnum', 'condition', 'stimlen','behavior']).size().reset_index(name='count')\n",
    "total_counts_headless = df_headless.groupby(['fnum','condition','stimlen']).size().reset_index(name='total_count')\n",
    "# Merge the behavior counts with total counts\n",
    "merged_headless = pd.merge(behavior_counts_headless, total_counts_headless, on=['fnum','stimlen', 'condition'])\n",
    "# Calculate the probability of each behavior at each frame\n",
    "merged_headless['probability'] = merged_headless['count'] / merged_headless['total_count']\n",
    "merged_headless['probability'] = merged_headless['probability'] *100\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(2.1, 1.5))\n",
    "sns.barplot(data=merged_headless[merged_headless.behavior=='kicking'], \n",
    "           x='condition', \n",
    "           y='probability',\n",
    "           #size=2,\n",
    "          # jitter=.25\n",
    "           #width=.5\n",
    "           #hue='opto', \n",
    "        #palette=['#780396'],\n",
    "           #errorbar=None,\n",
    "           #legend=False,\n",
    "           )\n",
    "# plt.xticks(np.arange(0, 2.1, 0.5))\n",
    "plt.yticks(np.arange(0.0, 20.1, 5))\n",
    "\n",
    "sns.despine(trim=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db714d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(2.1, 1.5))\n",
    "sns.barplot(data=merged_headless[merged_headless.behavior=='grooming'], \n",
    "           x='condition', \n",
    "           y='probability', \n",
    "          # width=.5\n",
    "           #hue='opto', \n",
    "        #palette=['#780396'],\n",
    "           #errorbar=None,\n",
    "           #legend=False,\n",
    "           )\n",
    "# plt.xticks(np.arange(0, 2.1, 0.5))\n",
    "plt.yticks(np.arange(0.0, 2.1, .5))\n",
    "\n",
    "sns.despine(trim=True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
